{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise Sheet 6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1\n",
    "\n",
    "The polynomial kernel $k(x, y)$ of degree $s$ is given by $\\left(x^{\\top} y+1\\right)^s$. Compute the corresponding mapping $\\Phi$ for the case that the data points $x$ are from $\\mathbb{R}$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find the corresponding mapping of $\\Phi$ for the kernel $k(x, y) = \\left(x^{\\top} y+1\\right)^s$:\n",
    "\n",
    "Scalar case where $x \\in \\mathbb{R}$:\n",
    "\n",
    " $k(x, y) = \\left(xy+1\\right)^s$\n",
    "\n",
    "To find the feature mapping $\\Phi(x)$, we need to expand this expression using the binomial theorem:\n",
    "$$\n",
    "(x y+1)^s=\\sum_{i=0}^s\\binom{s}{i}(x y)^i \\cdot 1^{s-i}=\\sum_{i=0}^s\\binom{s}{i} x^i y^i\n",
    "$$\n",
    "\n",
    "In this expansion, each term $x^i y^i$ corresponds to an interaction between the feature mappings of $x$ and $y$.\n",
    "\n",
    "From this, we can infer that the feature mapping $\\Phi$ that maps $x \\in \\mathbb{R}$ to a higher-dimensional space is given by:\n",
    "$$\n",
    "\\Phi(x)=\\left(1, x, x^2, x^3, \\ldots, x^s\\right)^{\\top}\n",
    "$$\n",
    "\n",
    "So, the $i$-th component of $\\Phi(x)$ is $x^{i-1}$ for $i=1,2, \\ldots, s+1$. This feature mapping transforms the scalar $x$ into a vector of polynomial terms up to degree $s$.\n",
    "\n",
    "Therefore, the explicit mapping $\\Phi(x)$ for the polynomial kernel $k(x, y)=(x y+1)^s$ when $x \\in \\mathbb{R}$ is\n",
    "$$\n",
    "\\Phi(x)=\\left(1, x, x^2, x^3, \\ldots, x^s\\right)^{\\top}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2\n",
    "\n",
    "Show that if $k_1$ and $k_2$ are two kernels and $\\alpha_1>0$ and $\\alpha_2>0$ are two scalars, then $k=\\alpha_1 k_1+\\alpha_2 k_2$ is also a kernel.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To show that  $k=\\alpha_1 k_1+\\alpha_2 k_2$ is a valid kernel, we need to demonstrate that $k$ satisfies the properties of a kernel function, particularly that it corresponds to an inner product in some feature space.\n",
    "\n",
    "#### Positive Semi-Definiteness\n",
    "A function $k(x, y)$ is a kernel if it defines a positive semi-definite kernel matrix $K$ for any set of data points. The kernel matrix $K$ for a set of data points $\\left\\{x_1, x_2, \\ldots, x_n\\right\\}$ is defined by $K_{i j}=$ $k\\left(x_i, x_j\\right)$. A kernel is positive semi-definite if for any set of real numbers $\\left\\{c_1, c_2, \\ldots, c_n\\right\\}$, the following holds:\n",
    "$$\n",
    "\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k\\left(x_i, x_j\\right) \\geq 0\n",
    "$$\n",
    "\n",
    "Given that $k_1$ and $k_2$ are kernels, their corresponding kernel matrices $K_1$ and $K_2$ are positive semidefinite. This means:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k_1\\left(x_i, x_j\\right) \\geq 0 \\\\\n",
    "& \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k_2\\left(x_i, x_j\\right) \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Linearity and Combination of Kernels\n",
    "Now consider $k=\\alpha_1 k_1+\\alpha_2 k_2$. The corresponding kernel matrix $K$ for $k$ is given by:\n",
    "$$\n",
    "K=\\alpha_1 K_1+\\alpha_2 K_2\n",
    "$$\n",
    "\n",
    "For $k$ to be a valid kernel, we need to show that:\n",
    "$$\n",
    "\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k\\left(x_i, x_j\\right)=\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j\\left(\\alpha_1 k_1\\left(x_i, x_j\\right)+\\alpha_2 k_2\\left(x_i, x_j\\right)\\right) \\geq 0\n",
    "$$\n",
    "\n",
    "Since $\\alpha_1$ and $\\alpha_2$ are positive scalars, we can distribute and combine the sums:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j\\left(\\alpha_1 k_1\\left(x_i, x_j\\right)+\\alpha_2 k_2\\left(x_i, x_j\\right)\\right)=\\alpha_1 \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k_1\\left(x_i, x_j\\right)+ \\\\\n",
    "& \\alpha_2 \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k_2\\left(x_i, x_j\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $k_1$ and $k_2$ are positive semi-definite kernels:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k_1\\left(x_i, x_j\\right) \\geq 0 \\\\\n",
    "& \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k_2\\left(x_i, x_j\\right) \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\alpha_1 \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k_1\\left(x_i, x_j\\right) \\geq 0 \\\\\n",
    "& \\alpha_2 \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k_2\\left(x_i, x_j\\right) \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Adding these two non-negative quantities together:\n",
    "$$\n",
    "\\alpha_1 \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k_1\\left(x_i, x_j\\right)+\\alpha_2 \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k_2\\left(x_i, x_j\\right) \\geq 0\n",
    "$$\n",
    "\n",
    "Thus, $\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k\\left(x_i, x_j\\right) \\geq 0$, provir $\\sim$ that $k=\\alpha_1 k_1+\\alpha_2 k_2$ is indeed a valid kernel."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Dataset 1:\n",
      "Best parameters: {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': 'rbf'}\n",
      "Training accuracy: 0.94\n",
      "Test accuracy: 0.8\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Dataset 2:\n",
      "Best parameters: {'svc__C': 10, 'svc__gamma': 1, 'svc__kernel': 'rbf'}\n",
      "Training accuracy: 0.92\n",
      "Test accuracy: 0.86\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Dataset 3:\n",
      "Best parameters: {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.7\n",
      "Dataset 1 - Training accuracy: 0.94, Test accuracy: 0.8\n",
      "Dataset 2 - Training accuracy: 0.92, Test accuracy: 0.86\n",
      "Dataset 3 - Training accuracy: 1.0, Test accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load your datasets here. For demonstration, I'll use placeholders.\n",
    "# Replace `load_your_dataset` with your actual data loading function.\n",
    "def load_data(name, m=None):\n",
    "    data = np.load(name)\n",
    "    x = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "def load_your_dataset(name: str):\n",
    "    x_train, y_train = load_data(f'{name}_train.npy')\n",
    "    x_test, y_test = load_data(f'{name}_test.npy')\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# Load datasets\n",
    "datasets = []\n",
    "for name in [\"dataset_O\", \"dataset_U\", \"dataset_V\"]:\n",
    "    X_train, y_train, X_test, y_test = load_your_dataset(name)\n",
    "    datasets.append((X_train, y_train, X_test, y_test))\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'svc__C': [0.1, 1, 10, 100],\n",
    "    'svc__gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'svc__kernel': ['rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Define the SVM pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Standardize the data\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "results = []\n",
    "\n",
    "# Perform grid search and cross-validation for each dataset\n",
    "for i, (X_train, y_train, X_test, y_test) in enumerate(datasets):\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    train_accuracy = accuracy_score(y_train, best_model.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, best_model.predict(X_test))\n",
    "\n",
    "    results.append((train_accuracy, test_accuracy))\n",
    "    print(f\"Dataset {i+1}:\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Training accuracy: {train_accuracy}\")\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# Display the final results\n",
    "for i, (train_accuracy, test_accuracy) in enumerate(results):\n",
    "    print(f\"Dataset {i+1} - Training accuracy: {train_accuracy}, Test accuracy: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T11:46:31.559982400Z",
     "start_time": "2024-05-26T11:46:28.650492400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
